{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd0301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meeting Minutes Audio Source\n",
    "This notebook uses an audio file downloaded from [this Google Drive link](https://drive.google.com/file/d/1N_kpSojRR5RYzupz6nqM8hMSoEF_R7pU/view?usp=sharing).\n",
    "The file has been downloaded and uploaded to Google Drive for processing in this notebook.\n",
    "\n",
    "**Instructions:**\n",
    "1. Download the audio file from the link above.\n",
    "2. Upload the file to your own Google Drive.\n",
    "3. Update the notebook to reference the correct path to your uploaded file (e.g., `/content/drive/MyDrive/your_folder/your_audio_file.mp3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573dd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for PyTorch (CUDA), Transformers, and OpenAI API.\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for file operations, API access, display, authentication, and model inference.\n",
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for audio transcription and LLM model names.\n",
    "AUDIO_MODEL = \"whisper-1\"\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and set the path to the uploaded audio file.\n",
    "drive.mount(\"/content/drive\")\n",
    "audio_filename = \"/content/drive/MyDrive/llms/denver_extract.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc18de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with HuggingFace Hub using a stored token.\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac51ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with OpenAI using a stored API key.\n",
    "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "openai = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5857d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the uploaded audio file to text using OpenAI Whisper model.\n",
    "audio_file = open(audio_filename, \"rb\")\n",
    "transcription = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file, response_format=\"text\")\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct system and user prompts for generating meeting minutes from transcript.\n",
    "system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
    "user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2798ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure quantization for efficient LLM inference using BitsAndBytes.\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ca82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize prompts, load the LLM, and generate meeting minutes with streaming output.\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
    "outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated output to readable text.\n",
    "response = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53814b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated meeting minutes in markdown format.\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
