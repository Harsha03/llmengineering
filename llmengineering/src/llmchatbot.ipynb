{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cdd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for LLM APIs, web scraping, environment management, and UI.\n",
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gradio for building the chatbot user interface.\n",
    "import gradio as gr # oh yeah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ad666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and print API key prefixes for debugging.\n",
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI, Anthropic, and Google Generative AI clients.\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1aff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system message for the chatbot's personality and response style.\n",
    "system_message = \"You are a snarky but helpful assistant that responds in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38161d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream responses from OpenAI GPT model using the system message and user prompt.\n",
    "def stream_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream responses from Anthropic Claude model using the system message and user prompt.\n",
    "def stream_claude(prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=system_message,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    response = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            response += text or \"\"\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream responses from Google Gemini model using the system message and user prompt.\n",
    "def stream_gemini(prompt):\n",
    "    model = google.generativeai.GenerativeModel(\"gemini-pro\")\n",
    "    response = model.generate_content(\n",
    "        [system_message, prompt],\n",
    "        generation_config={\"temperature\": 0.7, \"max_output_tokens\": 1000},\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        result += getattr(chunk, \"text\", \"\") or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b79871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream responses from the selected model (GPT, Claude, or Gemini) based on user input.\n",
    "def stream_model(prompt, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(prompt)\n",
    "    elif model==\"Gemini\":\n",
    "        result = stream_gemini(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and launch the Gradio interface for the chatbot, allowing users to select a model and interact.\n",
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\"), gr.Dropdown([\"GPT\", \"Claude\", \"Gemini\"], label=\"Select model\", value=\"GPT\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
